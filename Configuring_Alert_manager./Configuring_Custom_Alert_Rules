# fast read:
Here’s a structured way to add the steps for configuring custom alert rules for Blackbox Exporter and Alertmanager in the kube-prometheus-stack to your README:

---

## Configuring Custom Alert Rules for Blackbox Exporter and Alertmanager in Kube-Prometheus-Stack

### Step 1: Create a Custom Alert Rule File

1. Create the alert rule YAML file: Save the following configuration as `blackbox-exporter-rules.yaml`:

   ```yaml
   apiVersion: monitoring.coreos.com/v1
   kind: PrometheusRule
   metadata:
     name: blackbox-exporter-rules
     labels:
       prometheus: kube-prometheus
       role: alert-rules1
   spec:
     groups:
       - name: Blackbox_Exporter_rules
         rules:
           - alert: URLDown
             expr: probe_success == 0
             for: 1m
             labels:
               severity: critical
             annotations:
               summary: "A monitored URL is down"
               description: "A monitored URL of job has been down for more than 1 minute."
   ```

2. Apply the YAML file using `kubectl`:

   ```bash
   kubectl apply -f blackbox-exporter-rules.yaml
   ```

### Step 2: Update the `values.yml` File for the Helm Chart

1. Edit the `values.yml` file used for deploying the kube-prometheus-stack Helm chart.

2. Add or update the `ruleSelector` section to match the label in your custom alert rules:

   ```yaml
   prometheus:
     prometheusSpec:
       # Custom and default alert rules configuration
       ruleSelector:
         matchExpressions:
           - key: role
             operator: In
             values: ["alert-rules1"]
   ```

### Step 3: Configure Alertmanager in the `values.yml` File

1. To enable and configure Alertmanager, update the `values.yml` file with the following configuration:

   ```yaml
   alertmanager:
     enabled: true
     alertmanagerSpec:
       replicas: 1
       externalUrl: http://alertmanager-operated.monitoring.svc:9093
       config:
         global:
           smtp_smarthost: 'smtp.gmail.com:587'
           smtp_from: 'your-email@gmail.com'
           smtp_auth_username: 'your-email@gmail.com'
           smtp_auth_password: 'your-app-password'
           smtp_require_tls: true
         route:
           group_by: ['job']
           group_wait: 30s
           group_interval: 5m
           repeat_interval: 5m
           receiver: 'email'
           routes:
             - matchers:
                 - job = blackbox_exporter
               receiver: 'email'
         receivers:
           - name: 'email'
             email_configs:
               - to: 'recipient-email@gmail.com'
                 require_tls: true
                 send_resolved: false
         templates:
           - '/etc/alertmanager/config/*.tmpl'
   ```

### Step 4: Deploy or Update the Helm Chart

1. Deploy or update the Helm chart with the modified `values.yml`:

   ```bash
   helm upgrade prometheus-stack prometheus-community/kube-prometheus-stack -f values.yml --namespace monitoring
   ```

-----------------------------------------


# Detailed:
Here’s a more detailed version of the instructions for configuring custom alert rules for Blackbox Exporter and Alertmanager in the kube-prometheus-stack, covering every step in depth and including potential future actions or updates:

---

## Configuring Custom Alert Rules for Blackbox Exporter and Alertmanager in Kube-Prometheus-Stack

This guide will walk through creating and deploying custom alert rules for Blackbox Exporter using Prometheus and configuring Alertmanager to handle notifications, all within the kube-prometheus-stack Helm chart.

### Prerequisites

- **Helm**: Ensure you have Helm installed and the `kube-prometheus-stack` Helm chart already deployed in your Kubernetes cluster.
- **kubectl**: Install `kubectl` and ensure it's configured to interact with your cluster.
- **Blackbox Exporter**: Have Blackbox Exporter already deployed and configured in your Kubernetes cluster.
- **SMTP Credentials**: Ensure you have valid SMTP credentials (e.g., Gmail or other SMTP server) for configuring email alerts in Alertmanager.

### Step 1: Create a Custom Alert Rule File

Custom alert rules allow you to define specific conditions to monitor, in this case, Blackbox Exporter's `probe_success` metric to determine URL uptime.

1. **Create the Alert Rule YAML File**:
   Save the following alert rule configuration as `blackbox-exporter-rules.yaml`. This defines a Prometheus rule to alert if a monitored URL is down for more than one minute.

   ```yaml
   apiVersion: monitoring.coreos.com/v1
   kind: PrometheusRule
   metadata:
     name: blackbox-exporter-rules
     labels:
       prometheus: kube-prometheus
       role: alert-rules1
   spec:
     groups:
       - name: Blackbox_Exporter_rules
         rules:
           - alert: URLDown
             expr: probe_success == 0
             for: 1m
             labels:
               severity: critical
             annotations:
               summary: "A monitored URL is down"
               description: "A monitored URL of job has been down for more than 1 minute."
   ```

   **Explanation**:
   - `apiVersion`: Defines the API version for `PrometheusRule` (monitoring.coreos.com/v1).
   - `kind`: Specifies this is a `PrometheusRule` object.
   - `metadata`: Includes a name (`blackbox-exporter-rules`) and labels (`role: alert-rules1`) to identify the rule.
   - `spec`: Contains the alert rule configuration. This rule monitors the `probe_success` metric, triggering the alert if the URL is down for more than 1 minute (`for: 1m`).

2. **Apply the YAML File to Kubernetes**:
   Use `kubectl` to apply the alert rule file to your cluster:

   ```bash
   kubectl apply -f blackbox-exporter-rules.yaml
   ```

3. **Verify the Alert Rules**:
   Ensure that your custom rule is loaded into Prometheus by visiting the Prometheus UI (`/rules` endpoint) and checking that the `blackbox-exporter-rules` group is listed.

   **Future Steps**: If further custom alert rules are needed, you can add them to this file or create separate rule files and apply them using `kubectl`.

### Step 2: Update the `values.yml` File for the Helm Chart

Helm charts allow configuration through the `values.yml` file. To ensure that Prometheus picks up your custom alert rules, you need to update this file to include a `ruleSelector` that matches the labels in your custom alert rules.

1. **Edit the `values.yml` File**:
   In the `values.yml` file used for deploying `kube-prometheus-stack`, find or add the `ruleSelector` section under `prometheusSpec`:

   ```yaml
   prometheus:
     prometheusSpec:
       # Custom and default alert rules configuration
       ruleSelector:
         matchExpressions:
           - key: role
             operator: In
             values: ["alert-rules1"]
   ```

   **Explanation**:
   - `ruleSelector`: This section configures Prometheus to include custom alert rules based on their labels. In this case, it matches any rules with the label `role: alert-rules1`.
   - `matchExpressions`: Defines how labels should be matched (`key: role`, `operator: In`, `values: ["alert-rules1"]`).

2. **Apply the Changes**:
   After modifying the `values.yml` file, deploy or update the Helm chart with the changes.

   ```bash
   helm upgrade prometheus-stack prometheus-community/kube-prometheus-stack -f values.yml --namespace monitoring
   ```

   **Future Steps**: If new alert rules are added with different labels, you can modify the `ruleSelector` to include multiple label expressions.

### Step 3: Configure Alertmanager for Email Notifications

To handle alert notifications, you need to configure Alertmanager, which manages routing alerts to receivers (such as email, Slack, etc.). In this case, we will configure it to send email alerts when Blackbox Exporter detects that a monitored URL is down.

1. **Edit the `values.yml` File for Alertmanager**:
   In the same `values.yml` file, update the `alertmanager` section with the following configuration:

   ```yaml
   alertmanager:
     enabled: true
     alertmanagerSpec:
       replicas: 1
       externalUrl: http://alertmanager-operated.monitoring.svc:9093
       config:
         global:
           smtp_smarthost: 'smtp.gmail.com:587'
           smtp_from: 'your-email@gmail.com'
           smtp_auth_username: 'your-email@gmail.com'
           smtp_auth_password: 'your-app-password'
           smtp_require_tls: true
         route:
           group_by: ['job']
           group_wait: 30s
           group_interval: 5m
           repeat_interval: 5m
           receiver: 'email'
           routes:
             - matchers:
                 - job = blackbox_exporter
               receiver: 'email'
         receivers:
           - name: 'email'
             email_configs:
               - to: 'recipient-email@gmail.com'
                 require_tls: true
                 send_resolved: false
         templates:
           - '/etc/alertmanager/config/*.tmpl'
   ```

   **Explanation**:
   - `alertmanager.enabled`: Ensures Alertmanager is enabled in the deployment.
   - `smtp_smarthost`, `smtp_auth_username`, `smtp_auth_password`: These fields configure Alertmanager to send email alerts using Gmail's SMTP server. Use an App Password if you're using Gmail.
   - `route`: Defines how alerts are grouped and routed. Alerts where the `job` label is `blackbox_exporter` are routed to the `email` receiver.
   - `receivers`: Configures the email receiver to send alerts to `recipient-email@gmail.com`.
   - `templates`: Defines the directory for custom templates used by Alertmanager.

2. **Deploy or Update Alertmanager Configuration**:
   Deploy the updated Alertmanager configuration using Helm:

   ```bash
   helm upgrade prometheus-stack prometheus-community/kube-prometheus-stack -f values.yml --namespace monitoring
   ```

3. **Test the Alertmanager Configuration**:
   - Trigger a test alert by manually setting `probe_success` to `0` for one of the monitored URLs.
   - Verify that an email is sent to the configured recipient.

4. **Customize Alert Templates** (Optional):
   You can customize the alert messages by adding templates in the `/etc/alertmanager/config/*.tmpl` directory.

   **Future Steps**: As you add new alerting rules or receivers (e.g., Slack, PagerDuty), update the Alertmanager configuration with new `routes` and `receivers` as needed.

### Step 4: Regular Updates and Maintenance

1. **Monitor and Tune Alerts**:
   - Periodically review your alerts and tune their conditions, durations, and severities to avoid alert fatigue.
   - Ensure that your alerting rules cover critical services and components.

2. **Scaling and High Availability**:
   - If your cluster grows, consider increasing the number of Alertmanager replicas for high availability by updating `alertmanagerSpec.replicas`.
   - Similarly, scale Prometheus by adjusting the `prometheusSpec.replicas` field in the `values.yml` file.

3. **Add Additional Receivers**:
   - Expand the alerting system by configuring additional receivers like Slack, OpsGenie, PagerDuty, etc., in Alertmanager.
   - Update the `routes` section to route specific alerts to different receivers.

4. **Security and Compliance**:
   - Ensure SMTP credentials are kept secure, and use environment variables or Kubernetes Secrets for storing sensitive information like `smtp_auth_password`.
   - Regularly update your kube-prometheus-stack Helm chart to receive security updates and new features.

---

This detailed guide includes all the steps and considerations for configuring Blackbox Exporter alerts and Alertmanager notifications within the kube-prometheus-stack, along with potential future updates or configurations.
